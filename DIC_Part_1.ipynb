{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzvS8aZLNRru"
   },
   "source": [
    "\n",
    "\n",
    "<h1><center> CSE 487/587 Assignment 3: Predictive Analytics with Spark</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jONxKY8XRd8q"
   },
   "source": [
    "<h1><center> PART 1 - Basic Model<center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as774PSER08g"
   },
   "source": [
    "## Importing packages and checking the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpKLP-GScunn"
   },
   "outputs": [],
   "source": [
    "# Checking the JAVA version\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhq4rfE2VsAb"
   },
   "outputs": [],
   "source": [
    "# Import the findspark and pyspark\n",
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "n5bv8O3EcwdG",
    "outputId": "2c59b5ea-00fa-41b4-a1dd-b5a0d68a5ae6"
   },
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VGrI-s2xStB"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtzYtaYxnd0A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, size,array\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  Tokenizer, StopWordsRemover,HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pyspark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the max memory to 8gb RAM\n",
    "MAX_MEMORY = \"8g\"\n",
    "\n",
    "# Creating the spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvutqpCkTNmU"
   },
   "source": [
    "## Importing Train, Test and Mapping datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxYsCHu0x9Yo"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "mapping = pd.read_csv(\"mapping.csv\",names=[\"genre_id\", \"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OalJ1BNFZCjD"
   },
   "source": [
    "## Checking datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "tuo5MJj7x917",
    "outputId": "cde5834d-ed69-43dd-cbbe-e354220c0162"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "G0K34_WEY6yo",
    "outputId": "d28be376-32c3-4f27-8769-d4e65870f5dc"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "DYQFq-1PY-vE",
    "outputId": "d4a49362-b687-4ba7-febf-57d57dd040a0"
   },
   "outputs": [],
   "source": [
    "mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZMiUselZRzO"
   },
   "source": [
    "## Converting pandas datasets to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZDd14riPs4H"
   },
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(train)\n",
    "spark_test_df= sqlContext.createDataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "UerbTAVwAPGR",
    "outputId": "4ce8f743-338b-4bb7-db14-ef5ecc99f30c"
   },
   "outputs": [],
   "source": [
    "spark_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "oh60V7KGAJWM",
    "outputId": "ca4cf829-93eb-495a-d5b1-1f25f9ae82c5"
   },
   "outputs": [],
   "source": [
    "spark_test_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiZ-T9sIZnyz"
   },
   "source": [
    "## select particular columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "qb-JvKpsRudy",
    "outputId": "046a598b-5ec0-4f70-977e-236c166bece2"
   },
   "outputs": [],
   "source": [
    "spark_df.select(\"movie_id\",\"movie_name\",\"plot\",\"genre\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bP4Lt1xZ3VU"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4742kFFiCHbV"
   },
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "7p3XwpitopsE",
    "outputId": "fe5f7110-12b1-4bdc-9606-15227a762eae"
   },
   "outputs": [],
   "source": [
    "# Using pyspark.sql.functions regexp_replace to remove punctuations and special characters\n",
    "def clean_text(c):\n",
    "    c = lower(c)\n",
    "    c = regexp_replace(c, \"^rt \", \"\")\n",
    "    c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "    c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    return c\n",
    "\n",
    "# We will do this for only \"plot\" column of the dataframe\n",
    "# Making the change in the train data\n",
    "clean_text_df = spark_df.select((\"movie_id\"),(\"movie_name\"),clean_text(col(\"plot\")).alias(\"plot\"),(\"genre\"))\n",
    "# Making the change in the test data\n",
    "clean_test_df = spark_test_df.select((\"movie_id\"),(\"movie_name\"),clean_text(col(\"plot\")).alias(\"plot\"))\n",
    "clean_text_df.printSchema()\n",
    "clean_text_df.show(5)\n",
    "clean_test_df.printSchema()\n",
    "clean_test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zL47JwC1Tebq"
   },
   "source": [
    "## We will first create the pipeline model which defines the step by step approach towards feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4TwXUJSAkU0"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Tokenizer\n",
    "# This will separate each word of the plot into a list of words\n",
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "\n",
    "#Step 2 - Removing stopwords from the tokenized words\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stopwords)\n",
    "\n",
    "#Step 3 - Creating term frequency of the stopwords removed tokenized words so here we will be using hashingTF\n",
    "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\",numFeatures= 100)\n",
    "\n",
    "#Step 4 - Creating the pipeline for the above approach\n",
    "tfmodel_pipeline = Pipeline(stages =[tokenizer, stopwordsRemover,hashing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPt4xn0SNTim"
   },
   "source": [
    "## Preparing Feature data for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "5GgPTiCgUnqU",
    "outputId": "f797656e-fde3-4402-9a71-328ff7083ea1"
   },
   "outputs": [],
   "source": [
    "#Fitting the pipeline to the train data\n",
    "model1 = tfmodel_pipeline.fit(clean_text_df) \n",
    "# Transforming the train data and creating separate dataframe of it\n",
    "featurizedData = model1.transform(clean_text_df)\n",
    "featurizedData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnAxT1h4NWYF"
   },
   "source": [
    "## Preparing Feature data for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "bZIBi7zDpUIq",
    "outputId": "38c8d673-2e3b-40d3-abbb-cf718f6702f1"
   },
   "outputs": [],
   "source": [
    "#Fitting the pipeline to the train data\n",
    "model2 = tfmodel_pipeline.fit(clean_test_df) \n",
    "# Transforming the train data and creating separate dataframe of it\n",
    "testData = model2.transform(clean_test_df)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZE31BrLFhK2U"
   },
   "outputs": [],
   "source": [
    "# Selecting only necessary columns\n",
    "featurizedData= featurizedData.select(\"movie_id\",\"movie_name\",\"plot\",\"genre\",\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "Lr-Ozn9ihP5L",
    "outputId": "1e4437bc-9922-4c6e-c247-a1d170ca1d1c"
   },
   "outputs": [],
   "source": [
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnthtf70gj5e"
   },
   "source": [
    "## Label Processing start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kF2xm3HuQMM-"
   },
   "outputs": [],
   "source": [
    "# We will use mapping.csv to process our genrelist and convert it to the labels having binary number like string telling specifying the multilabels of the given movie plot\n",
    "mapping=mapping[1:]\n",
    "mapping_spark = sqlContext.createDataFrame(mapping)\n",
    "mapping_spark.toPandas().set_index('genre_id').T.to_dict()\n",
    "\n",
    "#taking the genre column in the list\n",
    "mapping_genre_list=mapping_spark.select(\"genre\").collect()\n",
    "\n",
    "#extracting the genre name into the list\n",
    "genreList=[]\n",
    "for i in range(0,mapping_spark.count()):\n",
    "    genreList.append(mapping_genre_list[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "PathOSK2dQ0M",
    "outputId": "acc2f1c5-6fdb-4759-9db6-115fd0b7e960"
   },
   "outputs": [],
   "source": [
    "genreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OaJD8vzdNQzi"
   },
   "outputs": [],
   "source": [
    "# This will convert the genre to the binary like string(1 and 0) specifying multilabels of the given movie_id\n",
    "def oneHotEncoding(x):\n",
    "    indexList=\" \"\n",
    "    for genre in genreList:\n",
    "        if genre in x:\n",
    "            indexList=indexList+\"1\"\n",
    "        else:\n",
    "            indexList=indexList+\"0\"\n",
    "    temp=(indexList.replace(\"\", \" \")[1: -1])\n",
    "    return temp.strip(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DKToMEvg2tR"
   },
   "source": [
    "## Mapping Label with feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "YTp82Dw1bWC6",
    "outputId": "492e9e8f-0fa5-4938-c44d-ad8abbeb280f"
   },
   "outputs": [],
   "source": [
    "myfunction = UserDefinedFunction(lambda x: oneHotEncoding(x), StringType())\n",
    "featurizedData = featurizedData.select(*[myfunction(col).alias(\"labels\") if col == \"genre\" else col for col in featurizedData.columns])\n",
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-lLuu07bh-E"
   },
   "outputs": [],
   "source": [
    "label=featurizedData.select(\"movie_id\",\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "Im7p6_RIbk0n",
    "outputId": "b4bb2e21-75b3-4166-f2c2-c71d106e5ce9"
   },
   "outputs": [],
   "source": [
    "label.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5X_1IfXiS09"
   },
   "source": [
    "## Splitting Label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfvvfoqMbn-D"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,IntegerType\n",
    "split_col = pyspark.sql.functions.split(label['labels'],' ')\n",
    "for i in range(0,len(mapping_genre_list)):\n",
    "    label = label.withColumn('label{}'.format(i), split_col.getItem(i).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "tTdYvG0cbqHW",
    "outputId": "bc52bf72-f24a-4694-a673-fec0eaf94d21"
   },
   "outputs": [],
   "source": [
    "label.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHNijhGKjL_C"
   },
   "source": [
    "## Joining Feature vector column with label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHjLR60FbuAh"
   },
   "outputs": [],
   "source": [
    "featurizedData=featurizedData.drop('labels')\n",
    "df  = featurizedData.join(label, label.movie_id == featurizedData.movie_id).drop(label.movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "pSsyoZfkbwls",
    "outputId": "6ed4bcd2-32db-453c-fb73-138b2d93bbac"
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLafqObdjWii"
   },
   "source": [
    "## Function for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAAdcMGyb10D"
   },
   "outputs": [],
   "source": [
    "def fit_and_classify(df, labelcol, featurescol):\n",
    "    RandomForest_classifier = RandomForestClassifier(labelCol = labelcol, featuresCol= featurescol,numTrees=3,maxDepth=2)\n",
    "    RandomForest_classifier_pipeline = Pipeline(stages=[RandomForest_classifier])\n",
    "    RandomForest_model = RandomForest_classifier_pipeline.fit(df)\n",
    "    return RandomForest_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4iI1ndXGMoK"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzVjY8ynb47J"
   },
   "outputs": [],
   "source": [
    "# We will train the model for each label column and take the prediction\n",
    "# As we are using Random Forest Classifier we will need to balance our data so that our model performs better\n",
    "# So here we will do undersamlpling to make the data balanced\n",
    "\n",
    "label_to_append = []\n",
    "for i in range(20):\n",
    "    filter_df = df.select('movie_id','rawFeatures','label{}'.format(i))\n",
    "    #Dividing the filtered_df to check for the ratio\n",
    "    major_df = filter_df.filter(filter_df['label{}'.format(i)] == 1) # Selecting the rows with labels only 1\n",
    "    minor_df = filter_df.filter(filter_df['label{}'.format(i)] == 0) # Selecting the rows with labels only 0\n",
    "    if minor_df.count() > major_df.count(): \n",
    "        ratio = minor_df.count() / major_df.count()\n",
    "        sampled_df = minor_df.sample(False, 1/ratio) # Sampling the data as per the ratio and setting withReplacement to False\n",
    "        final_df = sampled_df.union(major_df) # creating a final dataframe for our  random forest classifer model\n",
    "    else:\n",
    "        ratio = major_df.count() / minor_df.count()\n",
    "        sampled_df = major_df.sample(False, 1/ratio)# Sampling the data as per the ratio and setting withReplacement to False\n",
    "        final_df = sampled_df.union(minor_df) # creating a final dataframe for our  random forest classifer model\n",
    "    RandomForest_model = fit_and_classify(final_df, labelcol='label{}'.format(i), featurescol='rawFeatures')\n",
    "    predictions = RandomForest_model.transform(testData) # Getting the predictions\n",
    "    label_to_append.append(predictions.select('movie_id', 'prediction')) # Selecting only movie_id and prediction column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erYFLmXEb9YD"
   },
   "outputs": [],
   "source": [
    "# Preparing the header row for each of the predicted label column (so total 20 prediction column)\n",
    "\n",
    "for i in range(len(label_to_append)):\n",
    "    label_to_append[i] = label_to_append[i].withColumnRenamed('prediction','prediction{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqaT3mxzb_jK"
   },
   "outputs": [],
   "source": [
    "# Joining the data\n",
    "\n",
    "joined_df = label_to_append[0]\n",
    "for i in range(1,len(label_to_append)):\n",
    "    joined_df = joined_df.join(label_to_append[i],on = ['movie_id'], how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC7IsfXZmvOk"
   },
   "source": [
    "## Assembling the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmVpwc9HcDNQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols = [\"prediction0\",\"prediction1\",\"prediction2\",\"prediction3\",\"prediction4\",\"prediction5\",\"prediction6\",\"prediction7\",\"prediction8\",\"prediction9\",\"prediction10\",\"prediction11\",\"prediction12\",\"prediction13\",\"prediction14\",\"prediction15\",\"prediction16\",\"prediction17\",\"prediction18\",\"prediction19\"], outputCol = 'Predictions')\n",
    "joined_df = vecAssembler.transform(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "PBz1sojZcF2O",
    "outputId": "f3c652bc-3bb1-4f48-ccc2-15f9ec09d29a"
   },
   "outputs": [],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNNcIg9gcQE4"
   },
   "outputs": [],
   "source": [
    "result_df = joined_df.withColumn('predictions', \n",
    "                    sf.concat((sf.col('prediction0')).cast(IntegerType()),sf.lit(' '), sf.col('prediction1').cast(IntegerType()),sf.lit(' '), sf.col('prediction2').cast(IntegerType()),sf.lit(' '), sf.col('prediction3').cast(IntegerType()),sf.lit(' '), sf.col('prediction4').cast(IntegerType()),sf.lit(' '), sf.col('prediction5').cast(IntegerType()),sf.lit(' '), sf.col('prediction6').cast(IntegerType()),sf.lit(' '),sf.col('prediction7').cast(IntegerType()),sf.lit(' '), sf.col('prediction8').cast(IntegerType()),sf.lit(' '), sf.col('prediction9').cast(IntegerType()),sf.lit(' '), sf.col('prediction10').cast(IntegerType()),sf.lit(' '), sf.col('prediction11').cast(IntegerType()),sf.lit(' '), sf.col('prediction12').cast(IntegerType()),sf.lit(' '), sf.col('prediction13').cast(IntegerType()),sf.lit(' '),sf.col('prediction14').cast(IntegerType()),sf.lit(' '), sf.col('prediction15').cast(IntegerType()),sf.lit(' '), sf.col('prediction16').cast(IntegerType()),sf.lit(' '), sf.col('prediction17').cast(IntegerType()),sf.lit(' '), sf.col('prediction18').cast(IntegerType()),sf.lit(' '), sf.col('prediction19').cast(IntegerType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6kPUxxbc9SM"
   },
   "outputs": [],
   "source": [
    "result_df=result_df.select(\"movie_id\",\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "c2iTy7mZdAR7",
    "outputId": "3cfa2c4f-143b-42db-e831-d0f1fdccb133"
   },
   "outputs": [],
   "source": [
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qM5fYZBJmnf7"
   },
   "source": [
    "## Saving the result dataframe into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DR3vTRkZcWh"
   },
   "outputs": [],
   "source": [
    "result_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"DIC_Assignment_3_Part1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DIC_Part_1_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
